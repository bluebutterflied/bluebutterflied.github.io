I"<p>The working of a Artificial Neural Network can be observed in multiple ways. One way is to solve a practical use-case on it. The problem we are going to solve today is Customer-Churn problem of a Bank<code class="language-plaintext highlighter-rouge">(will the customer stay or go)</code>.</p>

<p>For this purpose, i’m going to use the available dataset on kaggle. Here is the link to <a href="https://www.kaggle.com/code/campusx/notebook8ad570467f">Churn Dataset</a> and also i’m going to use the notebook provided by kaggle.</p>

<p>Initially we are importing the necessary libraries including numpy and pandas. Then we are obtaining the path to the dataset csv file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save &amp; Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
</code></pre></div></div>

<p>Now read the data in the form of pandas dataframe,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df=pd.read_csv('/kaggle/input/credit-card-customer-churn-prediction/Churn_Modelling.csv')
</code></pre></div></div>

<p>Observe the dataset by using these functions,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df.describe()
df.head()
</code></pre></div></div>

<p>Dataset information will be displayed. Also see the un-nessary columns and drop them.e.g <strong>RowNumber,CustomerId and Surname are of no use for us</strong>,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df.drop(columns = ['RowNumber','CustomerId','Surname'],inplace=True)
</code></pre></div></div>

<p>Now count the total categorical values in <code class="language-plaintext highlighter-rouge">Geography</code> and <code class="language-plaintext highlighter-rouge">Gender</code> column</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df['Geography'].value_counts()
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df['Gender'].value_counts()
</code></pre></div></div>

<p>The next step is to get the dummies where possible,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df=pd.get_dummies(df,columns=['Geography','Gender'],drop_first=True)
</code></pre></div></div>

<p>You’ve your dataset in useful form now, perform the initial<code class="language-plaintext highlighter-rouge"> train-test-split</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X=df.drop(columns=['Exited'])
y=df['Exited'].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)
</code></pre></div></div>

<p>Then scale the values using <code class="language-plaintext highlighter-rouge">StandardScaler library</code>,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_train_trf=scaler.fit_transform(X_train)
X_test_trf=scaler.transform(X_test)
</code></pre></div></div>

<p>It is now time to make our first Artificial Neural Network, so do necessary importings of tensorflow and keras,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
</code></pre></div></div>

<p>Now make a sequential model and add hidden layers to it,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model=Sequential()
model.add(Dense(11,activation='relu',input_dim=11))
model.add(Dense(11,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
</code></pre></div></div>

<p>Check the model summary,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model.summary()
</code></pre></div></div>
:ET