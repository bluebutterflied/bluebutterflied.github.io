I"r<p>Transfer learning is also one of the most important topic in Deep-Learning/Machine-Learning. I would say it is like using others pre-trained models for your problem. Deep Learning models are difficult to train because they are data-hungry. By this i mean they require alot of data in order to achieve good performance. Therefore is alot of data you can say labelled-data is required.</p>

<p>Letâ€™s assume the example of a simple cat-dog classifier. Weâ€™ll need a huge data inorder to make a good classifier. One method is to scrap the data using various data-scrapping techniques but there is still an issue in this case. That data will be unlabelled, youâ€™ve to assign labels manually and for this purpose labor will be required which is costly. This is the first problem.</p>

<p>Second reason for using the Pre-Trained models is the time. Deep Learning models usually take a huge time to train so people donâ€™t prefer long trainings by theirselves. The solution of these two problems is using pre-trained models for your projects. One of the most famous dataset is ImageNet. It is a big dataset of images almost 1.4 million images and there are total of 1000 classes in this dataset. Many models are trained on this dataset. Rather than creating your model and training it on this dataset, you can use those pre-trained models.</p>

<p>There is one problem in using pre-trained models. What if the class you want to classify is not present in the dataset on which model is trained. Here comes the transfer learning. In transfer learning we use the pre-trained models and we re-train a portion of the model on our dataset. Basically in CNN, there are two major parts of the architecture. One is the convolution layer portion while the second one is the fully-connected layer portion. The first portion works for all the problems because it extract the primitive features which is same for all the problems/classification task. While the fully connected portion is used for classification mainly, so we retrain this portion on our required dataset according to the problem.</p>

<p>Transfer learning is done in two ways,</p>

<ul>
  <li>Feature Extraction Method</li>
  <li>Fine-Tuning Method</li>
</ul>

<p>In the first method, we only retrain the fully connected layer portion according to our need. While in the Fine-Tuning method, we retrain some of the last convolution layers and fully connected layers.</p>

<h2 id="transfer-learning-feature-extraction-without-using-data-augmentation">Transfer Learning Feature Extraction without using Data Augmentation</h2>

<p>First of all download the dataset in your notebook,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!kaggle datasets download -d salader/dogs-vs-cats
</code></pre></div></div>

<p>Then unzip the dataset,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import zipfile
zip_ref = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()
</code></pre></div></div>

<p>import tensorflow and necessary libraries,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import tensorflow
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense,Flatten
from keras.applications.vgg16 import VGG16
</code></pre></div></div>

<p>create a base model,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conv_base = VGG16(
    weights='imagenet',
    include_top = False,
    input_shape=(150,150,3)
)
</code></pre></div></div>

<p>check the model summary,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conv_base.summary()
</code></pre></div></div>

<p><img src="/assets/images/clt/transfer-learning-in-keras/1.png" alt="1" /></p>

<p>Then create and connect the rest of the model,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = Sequential()

model.add(conv_base)
model.add(Flatten())
model.add(Dense(256,activation='relu'))
model.add(Dense(1,activation='sigmoid'))```

check model summary,

</code></pre></div></div>
<p>model.summary()</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
![2](/assets/images/clt/transfer-learning-in-keras/2.png)

change the training parameter for the starting covolution layers,

</code></pre></div></div>
<p>conv_base.trainable=False</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
add the generators to efficiently use the resources,

</code></pre></div></div>
<p>train_ds = keras.utils.image_dataset_from_directory(
    directory = â€˜/content/trainâ€™,
    labels=â€™inferredâ€™,
    label_mode = â€˜intâ€™,
    batch_size=32,
    image_size=(150,150)
)</p>

<p>validation_ds = keras.utils.image_dataset_from_directory(
    directory = â€˜/content/testâ€™,
    labels=â€™inferredâ€™,
    label_mode = â€˜intâ€™,
    batch_size=32,
    image_size=(150,150)
)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
normalize the images,

</code></pre></div></div>
<p>def process(image,label):
    image = tensorflow.cast(image/255. ,tensorflow.float32)
    return image,label</p>

<p>train_ds = train_ds.map(process)
validation_ds = validation_ds.map(process)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
compile the model,

</code></pre></div></div>
<p>model.compile(optimizer=â€™adamâ€™,loss=â€™binary_crossentropyâ€™,metrics=[â€˜accuracyâ€™])</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
store the history in a variable,

</code></pre></div></div>
<p>history = model.fit(train_ds,epochs=10,validation_data=validation_ds)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
plot the accuracy and validation accuracy using this history variable,

</code></pre></div></div>
<p>import matplotlib.pyplot as plt</p>

<p>plt.plot(history.history[â€˜accuracyâ€™],color=â€™redâ€™,label=â€™trainâ€™)
plt.plot(history.history[â€˜val_accuracyâ€™],color=â€™blueâ€™,label=â€™validationâ€™)
plt.legend()
plt.show()```</p>

<p><img src="/assets/images/clt/transfer-learning-in-keras/3.png" alt="3" /></p>

<p>plot the loss and the validation loss,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plt.plot(history.history['loss'],color='red',label='train')
plt.plot(history.history['val_loss'],color='blue',label='validation')
plt.legend()
plt.show()
</code></pre></div></div>

<p><img src="/assets/images/clt/transfer-learning-in-keras/4.png" alt="4" /></p>
:ET