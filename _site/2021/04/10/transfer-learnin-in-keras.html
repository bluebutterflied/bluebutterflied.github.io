<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<!-- <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:400,400i,700&display=swap" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Nunito+Sans:600,600i,700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet"> 

<link rel="stylesheet" href="/assets/css/tailwind.css">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet" href="/assets/css/cards.css">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100&display=swap" rel="stylesheet">
<title>Transfer Learning in Keras | Fine Tuning VS Feature Extraction</title>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125808593-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125808593-1');
</script>



<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Transfer Learning in Keras Fine Tuning VS Feature Extraction | SandsOfTimes - SANDY (SandsOfTimes) on Computing, Learning and More …</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Transfer Learning in Keras Fine Tuning VS Feature Extraction" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Transfer learning is also one of the most important topic in Deep-Learning/Machine-Learning. I would say it is like using others pre-trained models for your problem. Deep Learning models are difficult to train because they are data-hungry. By this i mean they require alot of data in order to achieve good performance. Therefore is alot of data you can say labelled-data is required." />
<meta property="og:description" content="Transfer learning is also one of the most important topic in Deep-Learning/Machine-Learning. I would say it is like using others pre-trained models for your problem. Deep Learning models are difficult to train because they are data-hungry. By this i mean they require alot of data in order to achieve good performance. Therefore is alot of data you can say labelled-data is required." />
<meta property="og:site_name" content="SandsOfTimes - SANDY (SandsOfTimes) on Computing, Learning and More …" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-10T20:52:55+05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Transfer Learning in Keras Fine Tuning VS Feature Extraction" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Transfer Learning in Keras Fine Tuning VS Feature Extraction","dateModified":"2021-04-10T20:52:55+05:00","datePublished":"2021-04-10T20:52:55+05:00","description":"Transfer learning is also one of the most important topic in Deep-Learning/Machine-Learning. I would say it is like using others pre-trained models for your problem. Deep Learning models are difficult to train because they are data-hungry. By this i mean they require alot of data in order to achieve good performance. Therefore is alot of data you can say labelled-data is required.","url":"/2021/04/10/transfer-learnin-in-keras","mainEntityOfPage":{"@type":"WebPage","@id":"/2021/04/10/transfer-learnin-in-keras"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <header class="texture-blue">
    <div class="container"><div class="navbar">
	<ul>
		
		<a href="/"><li  >Home</li></a>
		
		<a href="/about"><li  >About</li></a>
		
		<a href="/learn"><li  >Contact</li></a>
		
	</ul>
</div></div><div class="container">
	<h1>Transfer Learning in Keras | Fine Tuning VS Feature Extraction</h1>
	<h4 class="post-description"></h4>
	<div class="post-date" style="margin-top:20px">
		Published on Apr 10, 2021
	</div>
	<ul class="post-tags"><li>Python</li><li>Deep Learning</li><li>Project</li></ul>
</div>
</header>
  <main>
    <div class="container">
      <div class="post-container">
          <p>Transfer learning is also one of the most important topic in Deep-Learning/Machine-Learning. I would say it is like using others pre-trained models for your problem. Deep Learning models are difficult to train because they are data-hungry. By this i mean they require alot of data in order to achieve good performance. Therefore is alot of data you can say labelled-data is required.</p>

<p>Let’s assume the example of a simple cat-dog classifier. We’ll need a huge data inorder to make a good classifier. One method is to scrap the data using various data-scrapping techniques but there is still an issue in this case. That data will be unlabelled, you’ve to assign labels manually and for this purpose labor will be required which is costly. This is the first problem.</p>

<p>Second reason for using the Pre-Trained models is the time. Deep Learning models usually take a huge time to train so people don’t prefer long trainings by theirselves. The solution of these two problems is using pre-trained models for your projects. One of the most famous dataset is ImageNet. It is a big dataset of images almost 1.4 million images and there are total of 1000 classes in this dataset. Many models are trained on this dataset. Rather than creating your model and training it on this dataset, you can use those pre-trained models.</p>

<p>There is one problem in using pre-trained models. What if the class you want to classify is not present in the dataset on which model is trained. Here comes the transfer learning. In transfer learning we use the pre-trained models and we re-train a portion of the model on our dataset. Basically in CNN, there are two major parts of the architecture. One is the convolution layer portion while the second one is the fully-connected layer portion. The first portion works for all the problems because it extract the primitive features which is same for all the problems/classification task. While the fully connected portion is used for classification mainly, so we retrain this portion on our required dataset according to the problem.</p>

<p>Transfer learning is done in two ways,</p>

<ul>
  <li>Feature Extraction Method</li>
  <li>Fine-Tuning Method</li>
</ul>

<p>In the first method, we only retrain the fully connected layer portion according to our need. While in the Fine-Tuning method, we retrain some of the last convolution layers and fully connected layers.</p>

<h2 id="transfer-learning-feature-extraction-without-using-data-augmentation">Transfer Learning Feature Extraction without using Data Augmentation</h2>

<p>First of all download the dataset in your notebook,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!kaggle datasets download -d salader/dogs-vs-cats
</code></pre></div></div>

<p>Then unzip the dataset,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import zipfile
zip_ref = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()
</code></pre></div></div>

<p>import tensorflow and necessary libraries,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import tensorflow
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense,Flatten
from keras.applications.vgg16 import VGG16
</code></pre></div></div>

<p>create a base model,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conv_base = VGG16(
    weights='imagenet',
    include_top = False,
    input_shape=(150,150,3)
)
</code></pre></div></div>

<p>check the model summary,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conv_base.summary()
</code></pre></div></div>

<p><img src="/assets/images/clt/transfer-learning-in-keras/1.png" alt="1" /></p>

<p>Then create and connect the rest of the model,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = Sequential()

model.add(conv_base)
model.add(Flatten())
model.add(Dense(256,activation='relu'))
model.add(Dense(1,activation='sigmoid'))```
</code></pre></div></div>
<p>check model summary,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model.summary()
</code></pre></div></div>

<p><img src="/assets/images/clt/transfer-learning-in-keras/2.png" alt="2" /></p>

<p>change the training parameter for the starting covolution layers,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conv_base.trainable=False
</code></pre></div></div>

<p>add the generators to efficiently use the resources,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_ds = keras.utils.image_dataset_from_directory(
    directory = '/content/train',
    labels='inferred',
    label_mode = 'int',
    batch_size=32,
    image_size=(150,150)
)

validation_ds = keras.utils.image_dataset_from_directory(
    directory = '/content/test',
    labels='inferred',
    label_mode = 'int',
    batch_size=32,
    image_size=(150,150)
)
</code></pre></div></div>

<p>normalize the images,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def process(image,label):
    image = tensorflow.cast(image/255. ,tensorflow.float32)
    return image,label

train_ds = train_ds.map(process)
validation_ds = validation_ds.map(process)
</code></pre></div></div>

<p>compile the model,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
</code></pre></div></div>

<p>store the history in a variable,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>history = model.fit(train_ds,epochs=10,validation_data=validation_ds)
</code></pre></div></div>

<p>plot the accuracy and validation accuracy using this history variable,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'],color='red',label='train')
plt.plot(history.history['val_accuracy'],color='blue',label='validation')
plt.legend()
plt.show()
</code></pre></div></div>

<p><img src="/assets/images/clt/transfer-learning-in-keras/3.png" alt="3" /></p>

<p>plot the loss and the validation loss,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plt.plot(history.history['loss'],color='red',label='train')
plt.plot(history.history['val_loss'],color='blue',label='validation')
plt.legend()
plt.show()
</code></pre></div></div>

<p><img src="/assets/images/clt/transfer-learning-in-keras/4.png" alt="4" /></p>

      </div>

        <!-- Configure Disqus --></div>
  </main><footer class="footer">
  <span class="text-muted"> SandsOfTime | No rights reserved.</span>
</footer>
</body>
</html>