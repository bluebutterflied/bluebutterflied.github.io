<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<!-- <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:400,400i,700&display=swap" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Nunito+Sans:600,600i,700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet"> 

<link rel="stylesheet" href="/assets/css/tailwind.css">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet" href="/assets/css/cards.css">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100&display=swap" rel="stylesheet">
<title>LSTM | Text Generation using Supervised Learning</title>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125808593-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125808593-1');
</script>



<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>LSTM Text Generation using Supervised Learning | SandsOfTimes - SANDY (SandsOfTimes) on Computing, Learning and More …</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="LSTM Text Generation using Supervised Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="LSTM are a pretty good replacement of RNNS’s. I’ll show the working of LSTM’s with a complete project. Our final model will take an input, whether it is a word or a phrase and predict the rest of the sentence by itself. Also you can ask any type of new query from the model and it will answer you. By some tinkering, i’ve made the model to act as a supervised learning model." />
<meta property="og:description" content="LSTM are a pretty good replacement of RNNS’s. I’ll show the working of LSTM’s with a complete project. Our final model will take an input, whether it is a word or a phrase and predict the rest of the sentence by itself. Also you can ask any type of new query from the model and it will answer you. By some tinkering, i’ve made the model to act as a supervised learning model." />
<meta property="og:site_name" content="SandsOfTimes - SANDY (SandsOfTimes) on Computing, Learning and More …" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-13T20:52:55+05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LSTM Text Generation using Supervised Learning" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"LSTM Text Generation using Supervised Learning","dateModified":"2021-04-13T20:52:55+05:00","datePublished":"2021-04-13T20:52:55+05:00","description":"LSTM are a pretty good replacement of RNNS’s. I’ll show the working of LSTM’s with a complete project. Our final model will take an input, whether it is a word or a phrase and predict the rest of the sentence by itself. Also you can ask any type of new query from the model and it will answer you. By some tinkering, i’ve made the model to act as a supervised learning model.","url":"/2021/04/13/lstm-new-word-or-sentence-predictor","mainEntityOfPage":{"@type":"WebPage","@id":"/2021/04/13/lstm-new-word-or-sentence-predictor"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <header class="texture-blue">
    <div class="container"><div class="navbar">
	<ul>
		
		<a href="/"><li  >Home</li></a>
		
		<a href="/about"><li  >About</li></a>
		
		<a href="/learn"><li  >Contact</li></a>
		
	</ul>
</div></div><div class="container">
	<h1>LSTM | Text Generation using Supervised Learning</h1>
	<h4 class="post-description"></h4>
	<div class="post-date" style="margin-top:20px">
		Published on Apr 13, 2021
	</div>
	<ul class="post-tags"><li>Python</li><li>Deep Learning</li><li>Project</li></ul>
</div>
</header>
  <main>
    <div class="container">
      <div class="post-container">
          <p>LSTM are a pretty good replacement of RNNS’s. I’ll show the working of LSTM’s with a complete project. Our final model will take an input, whether it is a word or a phrase and predict the rest of the sentence by itself. Also you can ask any type of new query from the model and it will answer you. By some tinkering, i’ve made the model to act as a supervised learning model.</p>

<p>Initially we need to create a dataset, for this purpose i’m using a dummy dataset of sentences, here it is,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>faqs = """
About the Training
What is the enrollment cost for the Data Analytics Guidance Program (DAGP 2024)?
The program operates on a subscription basis where you pay Rs 899 monthly.
How long is the entire program?
The full duration is 6 months, making the total fee Rs 899 * 6 = Rs 5400 approximately.
What is the curriculum for the training?
We will cover the following topics:
- Basic Python for Analytics
- Key Data Science Libraries
- Data Cleaning Techniques
- Database Management with SQL
- Math Concepts for Data Analytics
- Machine Learning Models
- Real-world ML Projects
- MLOps Overview
- Industry Case Studies
You can view the detailed syllabus here: https://dataschool.co.in/programs/DAGP-2024/curriculum.

Does this program cover NLP and Deep Learning?
No, Deep Learning and NLP are not part of this training.
What if I miss a live session? Will there be a recording?
Yes, all sessions are recorded, so you can access them if you miss one.
Where can I find the schedule?
You can view the program calendar here: https://dataschool.co.in/schedule/DAGP-2024.
How long are the live sessions?
Each session runs for about 1.5 to 2 hours.
What language will the instructor use?
The sessions are conducted in Hinglish.
How will I know about upcoming classes?
We’ll send email notifications before every scheduled session for paid participants.
Can non-technical participants join?
Yes, this course is beginner-friendly.
Can I join the course mid-way?
Yes, you can enroll at any point during the program.
Will I have access to previous lectures if I join late?
Yes, once you pay, you’ll gain access to all past lectures in the dashboard.
Do I need to submit assignments?
No, we provide solutions for self-assessment purposes.
Will the program include case studies?
Yes, we’ll work through real case studies.
How can I contact support?
For inquiries, email us at support@dataschool.co.in.

Payment/Registration Questions
Where should I make the payment?
All payments should be made on our official website: https://dataschool.co.in/payments.
Can I pay the full Rs 5400 in one go?
No, the payments are split into monthly installments.
If I pay mid-month, when is the next payment due?
Your next payment is due 30 days after your first payment date. For example, if you pay on the 10th, your next payment is due on the 10th of the following month.
Is there a refund policy if I don't like the course?
Yes, you have a 7-day refund period after making the payment.
What if I live outside India and can’t make the payment online?
Please email support@dataschool.co.in for assistance.

Post-Registration Queries
How long can I access the paid content?
You can access videos as long as your subscription is active. After completing all payments (totaling Rs 5400), you’ll have access until September 2025.
Why isn't lifetime access available?
Due to the low cost of the course, lifetime access is not offered.
Where can I go for help after a session?
You can fill out a form on your dashboard, and our team will arrange a 1-on-1 doubt-clearing session.
Can I ask questions related to sessions I missed from previous weeks?
Yes, simply select "previous week" in the doubt form.
What if I face payment issues while living abroad?
You can reach out to support@dataschool.co.in for help.

Certificate and Placement Queries
How do I qualify for a certificate?
Two conditions apply:
1. You must pay the full Rs 5400.
2. You need to complete all assessments.
I joined late—how do I pay for previous months?
Once you make your first payment, you will see links in your dashboard to pay for the earlier months.
Does the program offer job placement services?
While we provide placement support, it is not a job guarantee. The assistance includes:
- Sessions on portfolio building
- Soft skills workshops
- Interaction with industry experts
- Guidance on job search strategies.
"""
</code></pre></div></div>

<p>you can check the dataset via variable,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>faqs
</code></pre></div></div>

<p>now this data is in textual form but we cannot feed or use textual data to our model, so we need to convert it into vectors/numbers so import tokenizer and necessary libraries,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer 
</code></pre></div></div>

<p>create a tokenizer instance,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tokenizer = Tokenizer()
</code></pre></div></div>

<p>fit the tokenizer on given raw data,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tokenizer.fit_on_texts([faqs])
</code></pre></div></div>

<p>you can check the token words along with their index,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tokenizer.word_index
</code></pre></div></div>

<p>split the raw data on the basis of new line character <code class="language-plaintext highlighter-rouge">\n</code> and store all in a list,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input_sequences=[]
for sentence in faqs.split('\n'):
  # print(sentence)
  # print(tokenizer.texts_to_sequences([sentence])[0])
  tokenized_sentence=tokenizer.texts_to_sequences([sentence])[0]

  for i in range(1,len(tokenized_sentence)):
    input_sequences.append(tokenized_sentence[:i+1])
</code></pre></div></div>

<p>check for the sentence/sequence for the max length for zero padding of vectors,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>max_len=max([len(x) for x in input_sequences])
max_len
</code></pre></div></div>

<p>perform the padding,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_input_sequences=pad_sequences(input_sequences,maxlen=max_len,padding='pre')
padded_input_sequences
</code></pre></div></div>

<p>split the output and output features into x and y so to make a proper dataset,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x=padded_input_sequences[:,:-1] #except last colm
# padded_input_sequences[:,-1]
y=padded_input_sequences[:,-1]
</code></pre></div></div>

<p>check the shape of x and y,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(x.shape)
print(y.shape)
</code></pre></div></div>

<p>check the total token words,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>len(tokenizer.word_index)
</code></pre></div></div>

<p>convert the output scalar y into a vector,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from tensorflow.keras.utils import to_categorical
y=to_categorical(y,num_classes=283)
</code></pre></div></div>

<p>check the shape,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y.shape
</code></pre></div></div>

<p>now lets make our model, import the necessary libraries,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding,LSTM,Dense
</code></pre></div></div>

<p>create the model,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model=Sequential()
model.add(Embedding(283,100,input_length=56))
model.add(LSTM(150))
model.add(Dense(283,activation='softmax'))
</code></pre></div></div>

<p>compile the model,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
</code></pre></div></div>

<p>check the model summary,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model.summary()
</code></pre></div></div>

<p>fit the model on x and y,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model.fit(x,y,epochs=100)
</code></pre></div></div>

<p>lets take a word, tokenize it, do zero padding and predict the next word from it using the model,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
text="mail"

token_text=tokenizer.texts_to_sequences([text])[0]

padded_token_text=pad_sequences([token_text],maxlen=56,padding='pre')
pos=np.argmax(model.predict(padded_token_text))
for word,index in tokenizer.word_index.items():
  if index==pos:
    print(word)
</code></pre></div></div>

<p>inorder to predict a complete sentence, use for loop,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import time
import numpy as np
text="NLP and Deep Learning both"

for i in range(10):
  token_text=tokenizer.texts_to_sequences([text])[0]
  padded_token_text=pad_sequences([token_text],maxlen=56,padding='pre')
  pos=np.argmax(model.predict(padded_token_text))

  for word,index in tokenizer.word_index.items():
    if index==pos:
      text=text+ " "+ word
      print(text)
      time.sleep(2)
</code></pre></div></div>
<p><img src="/assets/images/clt/lstm-new-word-or-sentence-predictor/1.png" alt="1" /></p>

<p>another demo is,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import time
import numpy as np
text="total duration of the course"

for i in range(10):
  token_text=tokenizer.texts_to_sequences([text])[0]
  padded_token_text=pad_sequences([token_text],maxlen=56,padding='pre')
  pos=np.argmax(model.predict(padded_token_text))

  for word,index in tokenizer.word_index.items():
    if index==pos:
      text=text+ " "+ word
      print(text)
      time.sleep(2)
</code></pre></div></div>
<p><img src="/assets/images/clt/lstm-new-word-or-sentence-predictor/2.png" alt="2" /></p>

      </div>

        <!-- Configure Disqus --></div>
  </main><footer class="footer">
  <span class="text-muted"> SandsOfTime | No rights reserved.</span>
</footer>
</body>
</html>