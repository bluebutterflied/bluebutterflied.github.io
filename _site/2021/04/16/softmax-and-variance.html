<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<!-- <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:400,400i,700&display=swap" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Nunito+Sans:600,600i,700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet"> 

<link rel="stylesheet" href="/assets/css/tailwind.css">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet" href="/assets/css/cards.css">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100&display=swap" rel="stylesheet">
<title>Softmax Function and Variance in Python</title>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125808593-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125808593-1');
</script>



<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Softmax Function and Variance in Python | SandsOfTimes - SANDY (SandsOfTimes) on Computing, Learning and More …</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Softmax Function and Variance in Python" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Softmax function is one of the most widely used activation function in deep learning. Applying it on a data with high variance and cause problems. Therefore we most provide the scaled data with low variance to this activation function. The implementation of softmax is very simple. Here is few lines code showing the implementation of softmax," />
<meta property="og:description" content="Softmax function is one of the most widely used activation function in deep learning. Applying it on a data with high variance and cause problems. Therefore we most provide the scaled data with low variance to this activation function. The implementation of softmax is very simple. Here is few lines code showing the implementation of softmax," />
<meta property="og:site_name" content="SandsOfTimes - SANDY (SandsOfTimes) on Computing, Learning and More …" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-16T20:52:55+05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Softmax Function and Variance in Python" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Softmax Function and Variance in Python","description":"Softmax function is one of the most widely used activation function in deep learning. Applying it on a data with high variance and cause problems. Therefore we most provide the scaled data with low variance to this activation function. The implementation of softmax is very simple. Here is few lines code showing the implementation of softmax,","datePublished":"2021-04-16T20:52:55+05:00","url":"/2021/04/16/softmax-and-variance","dateModified":"2021-04-16T20:52:55+05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2021/04/16/softmax-and-variance"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <header class="texture-black">
    <div class="container"><div class="navbar">
	<ul>
		
		<a href="/"><li  >Home</li></a>
		
		<a href="/about"><li  >About</li></a>
		
		<a href="/learn"><li  >Contact</li></a>
		
	</ul>
</div></div><div class="container">
	<h1>Softmax Function and Variance in Python</h1>
	<h4 class="post-description"></h4>
	<div class="post-date" style="margin-top:20px">
		Published on Apr 16, 2021
	</div>
	<ul class="post-tags"><li>Python</li><li>Deep Learning</li></ul>
</div>
</header>
  <main>
    <div class="container">
      <div class="post-container">
          <p>Softmax function is one of the most widely used activation function in deep learning. Applying it on a data with high variance and cause problems. Therefore we most provide the scaled data with low variance to this activation function. The implementation of softmax is very simple. Here is few lines code showing the implementation of softmax,</p>

<p>First of all import all the necessary libraries,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np 
import matplotlib.pyplot as plt 
</code></pre></div></div>

<p>then here is the softmax,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def softmax(x):
  # Ensure numerical stability by subtracting the max value from each element
  e_x=np.exp(x-np.max(x))
  softmax_values=e_x/e_x.sum(axis=0) # Normalize
  percent_values=softmax_values*100 #convert to percent
  #Format each value to a string with 2 decimal places followed by a percent sign
  formatted_percent_values=[f"{value:.2f}%" for value in percent_values]
  return formatted_percent_values
</code></pre></div></div>

<p>you can check it by apply on any vector of any dimension,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>softmax(np.array([1,10]))
</code></pre></div></div>

<p><img src="/assets/images/clt/softmax-and-variance/1.png" alt="1" /></p>

<p>also variance is a big problem in deep learning. You’ve to make sure the vector/scalar passed to softmax function has low variance other wise, the softmax will push the values to both maximum and minimum extremes. Check the variance by using the built in function numpy function <code class="language-plaintext highlighter-rouge">var</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.array([10,20,30,40,50,60,70]).var()
</code></pre></div></div>

<p><img src="/assets/images/clt/softmax-and-variance/2.png" alt="2" /></p>

<p>values close to each other (with less difference) have small variance,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.array([1,2,3,4,5,6,7]).var()
</code></pre></div></div>

<p><img src="/assets/images/clt/softmax-and-variance/3.png" alt="3" /></p>

      </div>

        <!-- Configure Disqus --></div>
  </main><footer class="footer">
  <span class="text-muted"> SandsOfTime | No rights reserved.</span>
</footer>
</body>
</html>