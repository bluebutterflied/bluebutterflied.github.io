<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<!-- <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:400,400i,700&display=swap" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Nunito+Sans:600,600i,700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet"> 

<link rel="stylesheet" href="/assets/css/tailwind.css">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet" href="/assets/css/cards.css">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100&display=swap" rel="stylesheet">
<title>Forward Propagation in Multi-Layer-Perceptron</title>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125808593-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125808593-1');
</script>



<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Forward Propagation in Multi-Layer-Perceptron | SandsOfTime - SANDY (SandsOfTime) on Computing, Learning and More …</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Forward Propagation in Multi-Layer-Perceptron" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Generally Back-Propagation is the main algorithm to train the neural network but before it, we need to do a forward pass which is known as Forward Propagation. Inputs are given through input layer with the weights. Simple linear algebra which includes matrix multiplication is used." />
<meta property="og:description" content="Generally Back-Propagation is the main algorithm to train the neural network but before it, we need to do a forward pass which is known as Forward Propagation. Inputs are given through input layer with the weights. Simple linear algebra which includes matrix multiplication is used." />
<meta property="og:site_name" content="SandsOfTime - SANDY (SandsOfTime) on Computing, Learning and More …" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-09T20:52:55+05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Forward Propagation in Multi-Layer-Perceptron" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Forward Propagation in Multi-Layer-Perceptron","description":"Generally Back-Propagation is the main algorithm to train the neural network but before it, we need to do a forward pass which is known as Forward Propagation. Inputs are given through input layer with the weights. Simple linear algebra which includes matrix multiplication is used.","datePublished":"2021-02-09T20:52:55+05:00","url":"/2021/02/09/forward-propagation-in-multi-layer-perceptron","dateModified":"2021-02-09T20:52:55+05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2021/02/09/forward-propagation-in-multi-layer-perceptron"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <header class="texture-blue">
    <div class="container"><div class="navbar">
	<ul>
		
		<a href="/"><li  >Home</li></a>
		
		<a href="/about"><li  >About</li></a>
		
		<a href="/learn"><li  >Contact</li></a>
		
	</ul>
</div></div><div class="container">
	<h1>Forward Propagation in Multi-Layer-Perceptron</h1>
	<h4 class="post-description"></h4>
	<div class="post-date" style="margin-top:20px">
		Published on Feb 9, 2021
	</div>
	<ul class="post-tags"><li>Python</li><li>Deep Learning</li></ul>
</div>
</header>
  <main>
    <div class="container">
      <div class="post-container">
          <p>Generally <code class="language-plaintext highlighter-rouge">Back-Propagation</code> is the main algorithm to train the neural network but before it, we need to do a forward pass which is known as <code class="language-plaintext highlighter-rouge">Forward Propagation</code>. Inputs are given through input layer with the weights. Simple linear algebra which includes matrix multiplication is used.</p>

<p><img src="/assets/images/clt/forward-propagation/1.jpeg" alt="1" /></p>

<p>We need to know how a neural network predict. For this purpose we need to understand the flow. Initially you’ve to identify the <code class="language-plaintext highlighter-rouge">total trainable parameters</code>. Here in this case we have 4 inputs and 3 hiddens layers. Find the total connections,</p>

<p><img src="/assets/images/clt/forward-propagation/2.jpeg" alt="2" /></p>

<p>This forward pass will be used to check the prediction. In the back the dot product b/w the inputs and weights is taken place with addition with the bias.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input*weights+bias
</code></pre></div></div>
<p>Here is the full calculations,</p>

<p><img src="/assets/images/clt/forward-propagation/4.jpeg" alt="4" /></p>

<p>To understand the full flow in one go. I’ve solved a complete example with all mathematical calculations. You’ll get enough idea from it.</p>

<p><img src="/assets/images/clt/forward-propagation/new1.jpeg" alt="new1" /></p>

<p><img src="/assets/images/clt/forward-propagation/new2.jpeg" alt="new2" /></p>

<p>The ouput of first hidden layer will become input for the next hidden layer,</p>

<p><img src="/assets/images/clt/forward-propagation/3.jpeg" alt="3" /></p>

<p>Atlast sigmoid function will be applied on the output obtained from the output layer and we’ll have a value in 1x1 matrix.</p>

<p><img src="/assets/images/clt/forward-propagation/new3.jpeg" alt="new3" /></p>


      </div>

        <!-- Configure Disqus --></div>
  </main><footer class="footer">
  <span class="text-muted"> SandsOfTime | No rights reserved.</span>
</footer>
</body>
</html>