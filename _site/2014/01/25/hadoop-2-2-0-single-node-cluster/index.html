<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<!-- <link href="https://fonts.googleapis.com/css?family=Nunito+Sans:400,400i,700&display=swap" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Nunito+Sans:600,600i,700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet"> 

<link rel="stylesheet" href="/assets/css/tailwind.css">
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="stylesheet" href="/assets/css/cards.css">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100&display=swap" rel="stylesheet">
<title>Hadoop 2.2.0 - Single Node Cluster</title>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125808593-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125808593-1');
</script>



<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Hadoop 2.2.0 - Single Node Cluster | recluze - Nauman (recluze) on Computing, Philosophy and More …</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Hadoop 2.2.0 - Single Node Cluster" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We’re going to use the the Hadoop tarball we compiled earlier to run a pseudo-cluster. That means we will run a one-node cluster on a single machine. If you haven’t already read the tutorial on building the tarball, please head over and do that first." />
<meta property="og:description" content="We’re going to use the the Hadoop tarball we compiled earlier to run a pseudo-cluster. That means we will run a one-node cluster on a single machine. If you haven’t already read the tutorial on building the tarball, please head over and do that first." />
<meta property="og:site_name" content="recluze - Nauman (recluze) on Computing, Philosophy and More …" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2014-01-25T06:56:06+05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Hadoop 2.2.0 - Single Node Cluster" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Hadoop 2.2.0 - Single Node Cluster","dateModified":"2014-01-25T06:56:06+05:00","datePublished":"2014-01-25T06:56:06+05:00","description":"We’re going to use the the Hadoop tarball we compiled earlier to run a pseudo-cluster. That means we will run a one-node cluster on a single machine. If you haven’t already read the tutorial on building the tarball, please head over and do that first.","url":"/2014/01/25/hadoop-2-2-0-single-node-cluster/","mainEntityOfPage":{"@type":"WebPage","@id":"/2014/01/25/hadoop-2-2-0-single-node-cluster/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <header class="texture-blue">
    <div class="container"><div class="navbar">
	<ul>
		
		<a href="/"><li  >Home</li></a>
		
		<a href="/about"><li  >About</li></a>
		
		<a href="/learn"><li  >Courses</li></a>
		
	</ul>
</div></div><div class="container">
	<h1>Hadoop 2.2.0 - Single Node Cluster</h1>
	<h4 class="post-description"></h4>
	<div class="post-date" style="margin-top:20px">
		Published on Jan 25, 2014
	</div>
	<ul class="post-tags"><li>Hadoop</li><li>Tutorials</li></ul>
</div>
</header>
  <main>
    <div class="container">
      <div class="post-container">
          <p>We’re going to use the the Hadoop tarball we compiled earlier to run a pseudo-cluster. That means we will run a one-node cluster on a single machine. If you haven’t already read the tutorial on building the tarball, please head over and do that first.</p>

<blockquote>
  <p><a href="http://www.csrdu.org/nauman/2014/01/23/geting-started-with-hadoop-2-2-0-building/" title="Geting started with Hadoop 2.2.0 — Building">Geting started with Hadoop 2.2.0 — Building</a></p>
</blockquote>

<p>Start up your (virtual) machine and login as the user ‘hadoop’. First, we’re going to setup the essentials required to run Hadoop. By the way, if you are running a VM, I suggest you kill the machine used for building Hadoop and re-start from a fresh instance of Ubuntu to avoid any issues with compatibility later. For reference, the OS we are using is 64-bit Ubuntu 12.04.3 LTS.</p>

<!--more-->

<h2 id="environment-setup">Environment Setup</h2>

<p>First thing we need to do is create an RSA keypair for our user.</p>

<p>[sourcecode lang=”bash”]<br />
ssh-keygen -t rsa # Don’t enter a password, pick all defaults<br />
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br />
ssh localhost<br />
[/sourcecode]</p>

<p>We just created a key and added it to our user’s authorized keys so that we can do password-less login to our own machine. The last command above should login without asking for password.</p>

<p>Next, we want to call this system ‘master’. It’s a master of itself – this will come in handy in the future. We need to change the hostname and then configure our hosts file.</p>

<p>[sourcecode lang=”bash”]<br />
sudo hostname master<br />
sudo vi /etc/hostname<br />
[/sourcecode]</p>

<p>The <code class="language-plaintext highlighter-rouge">hostname</code> file should have a single line:</p>

<p>[sourcecode lang=”bash”]<br />
master<br />
[/sourcecode]</p>

<p>Next, modify <code class="language-plaintext highlighter-rouge">/etc/hosts</code> file and just after the <code class="language-plaintext highlighter-rouge">localhost</code> line, add an entry identifying ‘master’:</p>

<p>[sourcecode lang=”bash”]<br />
127.0.0.1 localhost<br />
192.168.56.101 master<br />
[/sourcecode]</p>

<p>This is assuming your machine has the IP address <code class="language-plaintext highlighter-rouge">192.168.56.101</code>. Make sure that this address is accessible from other machines. We will be using it for looking at some stats inshaallah.</p>

<p>Next, we install Java. OpenJDK 7 works fine.</p>

<p>[sourcecode lang=”bash”]<br />
sudo apt-get install -y openjdk-7-jdk<br />
[/sourcecode]</p>

<p>Now we can start setting up Hadoop. Copy the tarball over to master. You can use scp or winscp for that, or put it on a webserver and access it from there. After you have the compiled tar in hadoop user’s home folder on master, it’s time to extract and configure it.</p>

<p>[sourcecode lang=”bash”]<br />
cd /usr/local<br />
sudo tar zxf ~/hadoop-2.2.0.tar.gz<br />
sudo chown hadoop:hadoop -R hadoop-2.2.0/ # change ownership<br />
sudo ln -s hadoop-2.2.0 hadoop # create a symbolic link for future upgrades<br />
sudo chown hadoop:hadoop -R hadoop</p>

<h1 id="create-dfs-storage-location-and-set-permissions">create DFS storage location and set permissions</h1>
<p>sudo mkdir -p /app/hadoop/tmp<br />
sudo chown hadoop:hadoop /app/hadoop/tmp/ -R<br />
[/sourcecode]</p>

<p>Ok. That was easy. Now, let’s go ahead and append hadoop’s executable folders to our path definition. I made the changes in <code class="language-plaintext highlighter-rouge">/etc/environment</code> but you can also modify your <code class="language-plaintext highlighter-rouge">~/.bashrc</code> file. Your choice. Just append the following to your path definition:</p>

<p>[sourcecode lang=”bash”]<br />
:/usr/local/hadoop/bin:/usr/local/hadoop/sbin<br />
[/sourcecode]</p>

<p>Now, source the file to put it into effect:</p>

<p>[sourcecode lang=”bash”]<br />
. /etc/environment<br />
[/sourcecode]</p>

<h2 id="configuration">Configuration</h2>

<p>It is now time to create some configuration files. They are plenty but don’t worry. I’m going to try and explain them and they’re fairly straight forward – and they work as of today.</p>

<p>[sourcecode lang=”bash”]<br />
cd /usr/local/hadoop<br />
mkdir conf<br />
touch conf/core-site.xml<br />
touch conf/mapred-site.xml<br />
touch conf/hdfs-site.xml<br />
touch conf/yarn-site.xml<br />
touch conf/capacity-scheduler-site.xml<br />
touch conf/hadoop-env.sh<br />
touch conf/slaves<br />
[/sourcecode]</p>

<p>The first one <code class="language-plaintext highlighter-rouge">conf/core-site.xml</code> is pretty easy.</p>

<p>[sourcecode lang=”xml”]<br />
&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;<br />
&lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt;<br />
&lt;configuration&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br />
 &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;<br />
 &lt;description&gt;A base for other temporary directories.&lt;/description&gt;<br />
 &lt;/property&gt;</p>

<p>&lt;property&gt;<br />
 &lt;name&gt;fs.default.name&lt;/name&gt;<br />
 &lt;value&gt;hdfs://master:54310/&lt;/value&gt;<br />
 &lt;/property&gt;<br />
&lt;/configuration&gt;<br />
[/sourcecode]</p>

<p>The first property is of interest – well, both are but the second is left to the default. The first one tells where to put the Hadoop FS (meta) data. That’s the directory we created above. The rest will be put by Hadoop within subfolders. The second property <code class="language-plaintext highlighter-rouge">fs.default.name</code> tells Hadoop where to look for the HDFS. If you see the machine’s local filesystem when you later try to retrieve directory listing of HDFS, you will know that you messed this setting up. Notice the host ‘master’ over here. Port is best left to default.</p>

<p>Next file is <code class="language-plaintext highlighter-rouge">conf/mapred-site.xml</code>. It only has one setting:</p>

<p>[sourcecode lang=”xml”]<br />
&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;<br />
&lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt;<br />
&lt;configuration&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br />
 &lt;value&gt;master:54311&lt;/value&gt;<br />
 &lt;description&gt;The host and port that the MapReduce job tracker runs<br />
 at. If “local”, then jobs are run in-process as a single map<br />
 and reduce task.<br />
 &lt;/description&gt;<br />
 &lt;/property&gt;<br />
&lt;/configuration&gt;<br />
[/sourcecode]</p>

<p>The description basically says it all. The HDFS configurations are given in <code class="language-plaintext highlighter-rouge">hdfs-site.xml</code> is also straight forward as we are not doing any customization for now.</p>

<p>[sourcecode lang=”xml”]<br />
&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;<br />
&lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt;<br />
&lt;configuration&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt;<br />
 &lt;value&gt;hadoop&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;dfs.replication&lt;/name&gt;<br />
 &lt;value&gt;1&lt;/value&gt;<br />
 &lt;description&gt;Default block replication.<br />
 The actual number of replications can be specified when the file is created.<br />
 The default of 3 is used if replication is not specified.<br />
 &lt;/description&gt;<br />
 &lt;/property&gt;<br />
&lt;/configuration&gt;<br />
[/sourcecode]</p>

<p>The last one is the most detailed but is still pretty simple. This is the listing for <code class="language-plaintext highlighter-rouge">yarn-site.xml</code> which basically replaces Job Tracker and Task Tracker of MapReduce 1.</p>

<p>[sourcecode lang=”xml”]<br />
&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;<br />
&lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt;<br />
&lt;configuration&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;<br />
 &lt;value&gt;true&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.dispatcher.exit-on-error&lt;/name&gt;<br />
 &lt;value&gt;true&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;<br />
 &lt;value&gt;/user&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.application.classpath&lt;/name&gt;<br />
 &lt;value&gt;<br />
 $HADOOP_CONF_DIR,<br />
 $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,<br />
 $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,<br />
 $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,<br />
 $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*<br />
 &lt;/value&gt;<br />
 &lt;/property&gt;</p>

<p>&lt;property&gt;<br />
 &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;<br />
 &lt;value&gt;master:8030&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;<br />
 &lt;value&gt;master:8031&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;<br />
 &lt;value&gt;master:8032&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;<br />
 &lt;value&gt;master:8033&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.web-proxy.address&lt;/name&gt;<br />
 &lt;value&gt;master:8034&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;<br />
 &lt;value&gt;master:8088&lt;/value&gt;<br />
 &lt;/property&gt;<br />
&lt;/configuration&gt;<br />
[/sourcecode]</p>

<p>The bottom half is of somewhat importance to us right now. These are different port configurations for services offered by the YARN resource manager. Make a note of the last one. That’s the web front-end we can use to monitor our cluster.</p>

<p>Next, we put the following content in <code class="language-plaintext highlighter-rouge">conf/capacity-scheduler-site.xml</code>.</p>

<p>[sourcecode lang=”xml”]<br />
&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;<br />
&lt;?xml-stylesheet type=”text/xsl” href=”configuration.xsl”?&gt;<br />
&lt;configuration&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;<br />
 &lt;value&gt;0.1&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;<br />
 &lt;value&gt;default&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;/name&gt;<br />
 &lt;value&gt;100&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.root.default.user-limit-factor&lt;/name&gt;<br />
 &lt;value&gt;1&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt;<br />
 &lt;value&gt;default&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.root.default.maximum-capacity&lt;/name&gt;<br />
 &lt;value&gt;100&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.root.default.state&lt;/name&gt;<br />
 &lt;value&gt;RUNNING&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.root.default.acl_submit_applications&lt;/name&gt;<br />
 &lt;value&gt;*&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.root.default.acl_administer_queue&lt;/name&gt;<br />
 &lt;value&gt;*&lt;/value&gt;<br />
 &lt;/property&gt;<br />
 &lt;property&gt;<br />
 &lt;name&gt;yarn.scheduler.capacity.node-locality-delay&lt;/name&gt;<br />
 &lt;value&gt;-1&lt;/value&gt;<br />
 &lt;/property&gt;<br />
&lt;/configuration&gt;<br />
[/sourcecode]</p>

<p>The last file we want to configure is the environment file <code class="language-plaintext highlighter-rouge">conf/hadoop-env.sh</code>. This will be read by the different script and can be used to setup different environment variables specific to Hadoop.</p>

<p>[sourcecode lang=”bash”]<br />
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</p>

<p>export HADOOP_HOME=/usr/local/hadoop<br />
export HADOOP_CONF_DIR=/usr/local/hadoop/conf<br />
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true</p>

<p>export HADOOP_COMMON_HOME=/usr/local/hadoop<br />
export HADOOP_HDFS_HOME=/usr/local/hadoop<br />
export HADOO_MAPRED_HOME=/usr/local/hadoop<br />
export HADOOP_YARN_HOME=/usr/local/hadoop</p>

<p>export YARN_CONF_DIR=/usr/local/hadoop/conf<br />
[/sourcecode]</p>

<p>We also need another <code class="language-plaintext highlighter-rouge">.sh</code> file but that is the same as the above. So, let’s just copy it.</p>

<p>[sourcecode lang=”bash”]<br />
cp conf/hadoop-env.sh conf/yarn-env.sh<br />
[/sourcecode]</p>

<p>One final thing we need to do is to tell Hadoop about our slaves. For now, we have only one so put the following in <code class="language-plaintext highlighter-rouge">conf/slaves</code>:</p>

<p>[sourcecode lang=”bash”]<br />
master<br />
[/sourcecode]</p>

<p>Well, that’s done. Now, let’s see if we can execute it.</p>

<h2 id="cluster-startup">Cluster Startup</h2>

<p>Since we are only on one node, starting it up is pretty easy (although, to look at the official docs, you’d think this was rocket science).</p>

<p>First, we need to format our namenode.</p>

<p>[sourcecode lang=”bash”]<br />
hdfs namenode -format<br />
[/sourcecode]</p>

<p>The hdfs command is actually in <code class="language-plaintext highlighter-rouge">/user/local/hadoop/bin</code> but since we have that added in the path, this works fine. After this, we need to start our HDFS.</p>

<p>[sourcecode lang=”bash”]<br />
. /etc/environment<br />
start-dfs.sh</p>
<h1 id="alternative-commands-to-start-namenode-and-datanode">Alternative commands to start namenode and datanode</h1>
<h1 id="hadoop-daemonsh-config-hadoop_conf_dir-script-hdfs-start-namenode">hadoop-daemon.sh –config $HADOOP_CONF_DIR –script hdfs start namenode</h1>
<h1 id="hadoop-daemonsh-config-hadoop_conf_dir-script-hdfs-start-datanode">hadoop-daemon.sh –config $HADOOP_CONF_DIR –script hdfs start datanode</h1>
<p>[/sourcecode]</p>

<p>If at a later run, you get an error like this:</p>

<p><code class="language-plaintext highlighter-rouge">There appears to be a gap in the edit log. We expected txid 1, but got txid 705.</code></p>

<p>Append <code class="language-plaintext highlighter-rouge">-recover</code> to the namenode command above.</p>

<p>Check which resources are running by executing the Java ps command:</p>

<p>[sourcecode lang=”bash”]<br />
jps<br />
[/sourcecode]</p>

<p>You sould see a NameNode and DataNode along with JPS itself.</p>

<p>And now the YARN daemons.</p>

<p>[sourcecode lang=”bash”]<br />
yarn-daemon.sh –config $HADOOP_CONF_DIR start resourcemanager<br />
yarn-daemon.sh –config $HADOOP_CONF_DIR start nodemanager<br />
[/sourcecode]</p>

<p>JPS should now list a NameNode, a DataNode, a ResourceManager, a NodeManager and JPS itself. To test the HDFS, you can issue the following command:</p>

<p>[sourcecode lang=”bash”]<br />
hdfs dfs -ls /<br />
[/sourcecode]</p>

<p>It shouldn’t return anything at this point. If it lists your local system files, re-check the <code class="language-plaintext highlighter-rouge">fs.default.name</code> settings. If everything works fine, go ahead and try to see if you can run the hadoop examples.</p>

<h2 id="executing-an-example-job">Executing an Example Job</h2>

<p>Let’s first get some files to upload to our NFS. As usual, we will get a few files form the Gutenberg project. See details here: <a href="http://www.gutenberg.org">http://www.gutenberg.org</a></p>

<p>[sourcecode lang=”bash”]<br />
cd /tmp<br />
mkdir gutenberg<br />
cd gutenberg<br />
wget http://www.gutenberg.org/cache/epub/20417/pg20417.txt<br />
wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt<br />
wget http://www.gutenberg.org/cache/epub/4300/pg4300.txt<br />
[/sourcecode]</p>

<p>Now, let’s create a folder in our HDFS and upload the folder there.</p>

<p>[sourcecode lang=”bash”]<br />
hdfs dfs -mkdir -p /user/hadoop/<br />
hdfs dfs -copyFromLocal /tmp/gutenberg /user/hadoop/<br />
hdfs dfs -ls /user/hadoop/gutenberg<br />
[/sourcecode]</p>

<p>If you can see the three files listed properly, we’re all good to go here and we can now run the wordcount example on this.</p>

<p>[sourcecode lang=”bash”]<br />
cd /usr/local/hadoop<br />
find . -name *examples*.jar</p>
<h1 id="see-where-the-file-is-found-and-use-it-below">see where the file is found and use it below</h1>

<p>cp share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar ./<br />
hadoop jar hadoop-mapreduce-examples-2.2.0.jar wordcount /user/hadoop/gutenberg /user/hadoop/gutenberg-out<br />
[/sourcecode]</p>

<p>That should run for a bit and then produce something like the following at the end:</p>

<p>[sourcecode lang=”bash”]<br />
File System Counters<br />
 FILE: Number of bytes read=813183<br />
 FILE: Number of bytes written=4754129<br />
 FILE: Number of read operations=0<br />
 FILE: Number of large read operations=0<br />
 FILE: Number of write operations=0<br />
 HDFS: Number of bytes read=8241626<br />
 HDFS: Number of bytes written=0<br />
 HDFS: Number of read operations=24<br />
 HDFS: Number of large read operations=0<br />
 HDFS: Number of write operations=3<br />
 Map-Reduce Framework<br />
 Map input records=77931<br />
 Map output records=629172<br />
 Map output bytes=6076101<br />
 Map output materialized bytes=1459156<br />
 Input split bytes=352<br />
 Combine input records=629172<br />
 Combine output records=101113<br />
 Reduce input groups=0<br />
 Reduce shuffle bytes=0<br />
 Reduce input records=0<br />
 Reduce output records=0<br />
 Spilled Records=101113<br />
 Shuffled Maps =0<br />
 Failed Shuffles=0<br />
 Merged Map outputs=0<br />
 GC time elapsed (ms)=516<br />
 CPU time spent (ms)=0<br />
 Physical memory (bytes) snapshot=0<br />
 Virtual memory (bytes) snapshot=0<br />
 Total committed heap usage (bytes)=524660736<br />
 File Input Format Counters<br />
 Bytes Read=3671523<br />
 File Output Format Counters<br />
 Bytes Written=4753421<br />
[/sourcecode]</p>

<p>You can now do a usual <code class="language-plaintext highlighter-rouge">dfs -ls</code> on the output folder to check and then get the output using the following command</p>

<p>[sourcecode lang=”bash”]<br />
hdfs dfs -getmerge /user/hadoop/gutenberg-out/part-r-00000 /tmp/gutenberg-wordcount</p>
<h1 id="alternative-command">Alternative command</h1>
<h1 id="hdfs-dfs--copytolocal-userhadoopgutenberg-out-tmp">hdfs dfs -copyToLocal /user/hadoop/gutenberg-out /tmp/</h1>

<p>head /tmp/gutenberg-wordcount<br />
[/sourcecode]</p>

<p>The contents should look something like this:</p>

<p>[sourcecode lang=”bash”]<br />
“(Lo)cra” 1<br />
“1490 1<br />
“1498,” 1<br />
“35” 1<br />
“40,” 1<br />
“A 2<br />
“AS-IS”. 1<br />
“A_ 1<br />
“Absoluti 1<br />
“Alack! 1<br />
[/sourcecode]</p>

<p>And that’s it. How do you turn this single-node cluster to a multi-node cluster? That’s not difficult but you’ll have to wait a few days for that.</p>

<p>Let me know in the comments section if you face any problem. I might be able to point you in the right direction.</p>


      </div>

        <!-- Configure Disqus --></div>
  </main><footer class="footer">
  <span class="text-muted"> recluze | No rights reserved.</span>
</footer>
</body>
</html>